{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentCNN",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfg9OtBjVFVt",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1fa794d-01b5-4456-da85-657a712f5de8"
      },
      "source": [
        "#@title Google Drive {run: \"auto\"}\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "drive_folder = \"COMP550_SentimentAnalysis/analyses/out/\" # @param {type:\"string\"}\n",
        "drive_folder = os.path.join(\"/gdrive/My Drive/\", drive_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyudwkrDzwC2",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3faedd4-5c83-464c-fc65-7728d79e7699"
      },
      "source": [
        "# @title Install PyNLPIR\n",
        "!pip install pynlpir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pynlpir in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from pynlpir) (7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4f1bEPK3QnS",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e260da5b-1d61-4ad8-a99d-08b6376d6f78"
      },
      "source": [
        "# @title Download A0_utils\n",
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=10-0KscaGn-rMfyx4xj_aRuJLWHed_GfU', 'A0_utils.py', False)\n",
        "\n",
        "import importlib, A0_utils\n",
        "importlib.reload(A0_utils)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10-0KscaGn-rMfyx4xj_aRuJLWHed_GfU\n",
            "To: /content/A0_utils.py\n",
            "100%|██████████| 14.8k/14.8k [00:00<00:00, 6.76MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'A0_utils' from '/content/A0_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-wzCskkmhfR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b52fffc-a222-41eb-d708-9a5147117e6e"
      },
      "source": [
        "# @title Download trainining, validation, and test sets\n",
        "import shutil\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "os.makedirs('input/train_valid_test/', exist_ok=True)\n",
        "os.makedirs('input/translated_train_valid_test/', exist_ok=True)\n",
        "\n",
        "filenames_urls = \\\n",
        "{\n",
        "  \"input/translated_train_valid_test.tar.gz\": \"https://drive.google.com/uc?id=1SOgaYjMnaQdE2FC5b8kJ-nkMAJbaZg1r\",\n",
        "  \"input/short_train_valid_test.tar.gz\": \"https://drive.google.com/uc?id=1t2NY3MHY0WvvlxkyG_B0XvTnosZ_qo51\",\n",
        "}\n",
        "# filenames_urls = {\n",
        "#   \"input/train_valid_test/train_amazon_com.csv\": \"https://drive.google.com/uc?id=11U_GLfigjC8B4dfXYFU0IqaVHtUlcBl0\",\n",
        "#   \"input/train_valid_test/valid_amazon_com.csv\": \"https://drive.google.com/uc?id=1vh94sOv7inWi88jn8Bqc_7MrPej8DyvX\",\n",
        "#   \"input/train_valid_test/test_amazon_com.csv\": \"https://drive.google.com/uc?id=1Wa22vKeMORmMTJ8Ci1xY-yYyLUlT137n\",\n",
        "#   \"input/train_valid_test/train_amazon_fr.csv\": \"https://drive.google.com/uc?id=1pFyPkizNxCXXNG03B3jhsxSfAJo3iemV\",\n",
        "#   \"input/train_valid_test/valid_amazon_fr.csv\": \"https://drive.google.com/uc?id=1pAI4S0Hdxysm3zq0rOllUF0j6woAfG_s\",\n",
        "#   \"input/train_valid_test/test_amazon_fr.csv\": \"https://drive.google.com/uc?id=1ecfNsTocUBsHlrauQ1dnwn6Mjt0A796t\",\n",
        "#   \"input/train_valid_test/train_amazon_es.csv\": \"https://drive.google.com/uc?id=16WyzjSpDnCdUHUcQjuIZwGYlsbg1Kizt\",\n",
        "#   \"input/train_valid_test/valid_amazon_es.csv\": \"https://drive.google.com/uc?id=1BXE4ACEQJ9Gpu2cquxrNNjtRJL1S8e2h\",\n",
        "#   \"input/train_valid_test/test_amazon_es.csv\": \"https://drive.google.com/uc?id=1qJJE7Hyj7zN0pygYvYrgCn5B5ERVLdvo\",\n",
        "#   \"input/train_valid_test/train_amazon_cn.csv\": \"https://drive.google.com/uc?id=1yNiombiBFSyE6Otpf4TjIeb7-isDKhO8\",\n",
        "#   \"input/train_valid_test/valid_amazon_cn.csv\": \"https://drive.google.com/uc?id=139LFaDkjvi3kGSuOMBLxyf2BMPfAs3Rm\",\n",
        "#   \"input/train_valid_test/test_amazon_cn.csv\": \"https://drive.google.com/uc?id=1qfBSmoSgeTS6JidZMtHcSjwbJcDC_xbg\",\n",
        "#   \"input/train_valid_test/train_translated_amazon_fr.csv\": \"https://drive.google.com/uc?id=1hezx8Rekg49C6IvkNs_8x5FyvoTtINRE\",\n",
        "#   \"input/train_valid_test/valid_translated_amazon_fr.csv\": \"https://drive.google.com/uc?id=1wWqJJ-jfOKmyEmMkhA1fDMYbzCYbUFtx\",\n",
        "#   \"input/train_valid_test/test_translated_amazon_fr.csv\": \"https://drive.google.com/uc?id=18pEm3CDktBipXB3UvITU3psqO0R62Cvi\",\n",
        "#   \"input/train_valid_test/train_translated_amazon_es.csv\": \"https://drive.google.com/uc?id=1TPqqFy9BmFu2SFmya--7WgpqB3qvIIlK\",\n",
        "#   \"input/train_valid_test/valid_translated_amazon_es.csv\": \"https://drive.google.com/uc?id=18txUuhEl264sBxO1RhC8jNbDhxGN6YR8\",\n",
        "#   \"input/train_valid_test/test_translated_amazon_es.csv\": \"https://drive.google.com/uc?id=1mRytiY799czZEcirbb2G0c5OfSwCdb7k\",\n",
        "#   \"input/train_valid_test/train_translated_amazon_cn.csv\": \"https://drive.google.com/uc?id=1f1ME7dAru2VyS8-w_hRu24i5XVVT4soY\",\n",
        "#   \"input/train_valid_test/valid_translated_amazon_cn.csv\": \"https://drive.google.com/uc?id=1fz59CBiWPCJxFAManjxNY-N8v4xPRbKi\",\n",
        "#   \"input/train_valid_test/test_translated_amazon_cn.csv\": \"https://drive.google.com/uc?id=1K65DgkjOPQSd6_gnntvIcy2QvCRMfJpt\"\n",
        "# }\n",
        "\n",
        "for filename, url in filenames_urls.items():\n",
        "  gdown.download(url, filename, False)\n",
        "shutil.move(\"input/train_valid_test\", \"input/short_train_valid_test\")\n",
        "\n",
        "for filename in filenames_urls:\n",
        "  shutil.unpack_archive(filename, extract_dir=\"input/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'input/short_train_valid_test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AH7LNO05e1C",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d5cf8985-907e-45c3-9d3b-837965d83738"
      },
      "source": [
        "# @title Download gensim models\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"input\"):\n",
        "  os.mkdir(\"input\")\n",
        "if not os.path.exists(\"input/gensim\"):\n",
        "  os.mkdir(\"input/gensim\")\n",
        "\n",
        "filenames_urls = {\n",
        "    \"input/gensim/amazon-com.wv\": \"https://drive.google.com/uc?id=1XJa4V8pTkw4otHkfDzdliqO3l5VL87_a\",\n",
        "    \"input/gensim/amazon-fr.wv\": \"https://drive.google.com/uc?id=1FdyS9VLIIs_800aggg8oeDWOncrhlToF\",\n",
        "    \"input/gensim/amazon-es.wv\": \"https://drive.google.com/uc?id=1Zalt98SloDLiZoXlbscCYKAZ-_OCa1WT\",\n",
        "    \"input/gensim/amazon-cn.wv\": \"https://drive.google.com/uc?id=17hudrssVI0armKqsy2pEv2121FT_bTfK\",\n",
        "    # \"input/gensim/text8-english.wv\": \"https://drive.google.com/uc?id=16WV0na21Uin_fXaZB3LK4uDq4xfRZyrP\",\n",
        "    # \"input/gensim/text8-amazon-com.wv\": \"https://drive.google.com/uc?id=18wu9-yqz3ZV1sz1FwIWfySkWyrXcBEqK\"\n",
        "}\n",
        "\n",
        "for filename, url in filenames_urls.items():\n",
        "  gdown.download(url, filename, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XJa4V8pTkw4otHkfDzdliqO3l5VL87_a\n",
            "To: /content/input/gensim/amazon-com.wv\n",
            "23.3MB [00:00, 146MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FdyS9VLIIs_800aggg8oeDWOncrhlToF\n",
            "To: /content/input/gensim/amazon-fr.wv\n",
            "17.6MB [00:00, 121MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zalt98SloDLiZoXlbscCYKAZ-_OCa1WT\n",
            "To: /content/input/gensim/amazon-es.wv\n",
            "18.9MB [00:00, 109MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17hudrssVI0armKqsy2pEv2121FT_bTfK\n",
            "To: /content/input/gensim/amazon-cn.wv\n",
            "6.47MB [00:00, 143MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM_iSugRC-Ga",
        "cellView": "form"
      },
      "source": [
        "# @title plot_results_to_grid()\n",
        "\n",
        "def plot_results_to_grid(\n",
        "    grid, where, title,\n",
        "    results, model_name, model_color,\n",
        "    learning_rate, lr_color\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    with grid.output_to(where[0], where[1]):\n",
        "        res_y = results\n",
        "        res_x = np.where(np.invert(np.isnan(res_y)))[0]\n",
        "        res_y = res_y[res_x]\n",
        "        res_x += 1\n",
        "\n",
        "        learning_rate = np.array(learning_rate)\n",
        "        lr_x = np.where((learning_rate[0:-1] != learning_rate[1:]) * np.invert(np.isnan(learning_rate[0:-1])) * np.invert(np.isnan(learning_rate[1:])))\n",
        "        lr_x = np.repeat(2+np.reshape(lr_x[0], (1, -1)), 2, 0)\n",
        "        lr_y = np.repeat(np.array([[0], [100]], ndmin=2), lr_x.shape[1], 1)\n",
        "        lr_r = learning_rate[lr_x[0, :] -1]\n",
        "\n",
        "        # Show original lr\n",
        "        lr_x = np.concatenate((np.array([[0.5],[0.5]], ndmin=2), lr_x), 1)\n",
        "        lr_y = np.concatenate((np.array([[np.nan],[np.nan]], ndmin=2), lr_y), 1)\n",
        "        lr_r = np.concatenate((np.array(learning_rate[0], ndmin=1), lr_r))\n",
        "\n",
        "        # Alternate text y position to make it more readable       \n",
        "        lr_t = np.repeat([[80], [20]], (1+lr_r.size)/2, 1).T.flatten()\n",
        "        \n",
        "        grid.clear_cell()\n",
        "\n",
        "        res_plt = plt.plot(res_x, res_y, model_color + '-', res_x, res_y, model_color + 'o')\n",
        "        lr_plt = plt.plot(lr_x, lr_y, lr_color + '--')\n",
        "        for x, r, t in zip(lr_x.T, lr_r, lr_t):\n",
        "            plt.text(x[0] + results.shape[0]/100, t, \"lr = {}\".format(r), color=lr_color) #withdash=True, \n",
        "        \n",
        "        # plt.legend((one_plt[0], two_plt[0]), (model_1_name, model_2_name))\n",
        "        plt.legend([res_plt[0]], [model_name])\n",
        "        plt.title(title)\n",
        "        plt.xlabel('# of epochs')\n",
        "        plt.xlim(0.5, 0.5 + results.shape[0])\n",
        "        plt.xticks(np.floor(np.linspace(1, results.shape[0], 11)))\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.ylim(0, 100)\n",
        "        plt.yticks(np.linspace(0, 100, 11));\n",
        "        plt.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhEMqSpqbkJa",
        "cellView": "form"
      },
      "source": [
        "# @title Word2Ix\n",
        "# Transform documents into lists of indices to the word vectors.\n",
        "def Word2Ix(document_set, word_vectors, pad_to_length):\n",
        "  document_set.vec = []\n",
        "  for doc in document_set.preprocessed:\n",
        "    document_set.vec.append(\n",
        "      torch.tensor(\n",
        "        [\n",
        "          word_vectors.vocab[word].index\n",
        "          if word in word_vectors.vocab else len(word_vectors.vocab)-1\n",
        "          for word in doc[:pad_to_length]\n",
        "        ] + [len(word_vectors.vocab)-1] * (pad_to_length - len(doc))\n",
        "      )\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I43lRT7n3qf5",
        "cellView": "code"
      },
      "source": [
        "# @title train, validate, and test\n",
        "from A0_utils import CsvDataset, SentimentLSTM, SentimentCNN, TrainNN, ValidateNN, TestNN\n",
        "from google.colab import widgets\n",
        "import logging\n",
        "import gensim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import string\n",
        "import joblib\n",
        "import math\n",
        "import os\n",
        "\n",
        "DEVICE=\"cuda\"\n",
        "NB_EPOCHS=10\n",
        "BATCH_SIZE=128\n",
        "SCH_STEP_SIZE=20\n",
        "SCH_GAMMA=0.5\n",
        "\n",
        "branches_wordvectors = \\\n",
        "[\n",
        "  # (\"short.amazon.com\", \"amazon-com.wv\"),\n",
        "  # (\"short.amazon.fr\", \"amazon-fr.wv\"),\n",
        "  # (\"short.amazon.es\", \"amazon-es.wv\"),\n",
        "  # (\"short.amazon.cn\", \"amazon-cn.wv\"),\n",
        "  (\"translated.amazon.fr\", \"amazon-com.wv\"),\n",
        "  (\"translated.amazon.es\", \"amazon-com.wv\"),\n",
        "  (\"translated.amazon.cn\", \"amazon-com.wv\")\n",
        "]\n",
        "\n",
        "grid = widgets.Grid(2 + len(branches_wordvectors), 1)\n",
        "for ix, (branch, wvname) in enumerate(branches_wordvectors):\n",
        "  for binary in [True, False]:\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    with grid.output_to(0, 0):\n",
        "      print(\"{}: {}\".format( branch, \"binary\" if binary else \"1-5\"))\n",
        "\n",
        "    index = branch.find('.amazon')\n",
        "    prefix = (branch[:index] + \"_\") if index != -1 else ''\n",
        "    branch_name = branch[1+index:]\n",
        "\n",
        "    # Load trainining, validation, and test sets\n",
        "    train_set = CsvDataset(\n",
        "      \"input/{prefix}train_valid_test/{prefix}train_{branch}.csv\".format(\n",
        "          prefix=prefix, branch=branch_name.replace('.', '_')\n",
        "      ), lambda c: c[\"preprocessed\"].split(), lambda c: int(c[\"labels\"])\n",
        "    )\n",
        "    valid_set = CsvDataset(\n",
        "      \"input/{prefix}train_valid_test/{prefix}valid_{branch}.csv\".format(\n",
        "          prefix=prefix, branch=branch_name.replace('.', '_')\n",
        "      ), lambda c: c[\"preprocessed\"].split(), lambda c: int(c[\"labels\"])\n",
        "    )\n",
        "    test_set = CsvDataset(\n",
        "      \"input/{prefix}train_valid_test/{prefix}train_{branch}.csv\".format(\n",
        "          prefix=prefix, branch=branch_name.replace('.', '_')\n",
        "      ), lambda c: c[\"preprocessed\"].split(), lambda c: int(c[\"labels\"])\n",
        "    )\n",
        "    \n",
        "    # Gensim model/word vectors using train_set text\n",
        "    word_vectors = gensim.models.KeyedVectors.load(\n",
        "      \"input/gensim/{}\".format(wvname), mmap='r'\n",
        "    )\n",
        "    # word_vectors = gensim.models.KeyedVectors.load(\n",
        "    #   \"input/gensim/amazon-com.wv\"\n",
        "    # )\n",
        "    #   Add <unk> token\n",
        "    word_vectors.add('<unk>', np.full(word_vectors.vector_size, 0))\n",
        "    \n",
        "    # Find longest document in training set.\n",
        "    longest_document_length = np.sort([len(tokens) for tokens in train_set.preprocessed])\n",
        "    longest_document_length = longest_document_length[round(len(longest_document_length) * 0.99)]\n",
        "    \n",
        "    # Word to word vector index (the->1, office->223, ...)\n",
        "    Word2Ix(train_set, word_vectors, longest_document_length)\n",
        "    Word2Ix(valid_set, word_vectors, longest_document_length)\n",
        "    Word2Ix(test_set, word_vectors, longest_document_length)\n",
        "    \n",
        "    if binary:\n",
        "      # Binarize labels\n",
        "      train_set.labels = [int(label == 5) for label in train_set.labels]\n",
        "      valid_set.labels = [int(label == 5) for label in valid_set.labels]\n",
        "      test_set.labels = [int(label == 5) for label in test_set.labels]\n",
        "    else:\n",
        "      # Labels must be 0-4 rather than 1-5\n",
        "      train_set.labels = [label-1 for label in train_set.labels]\n",
        "      valid_set.labels = [label-1 for label in valid_set.labels]\n",
        "      test_set.labels = [label-1 for label in test_set.labels]\n",
        "    \n",
        "    # Loaders\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # LSTM\n",
        "    model = SentimentCNN(\n",
        "      num_classes=len(set(train_set.labels)),\n",
        "      vocab_size=word_vectors.vectors.shape[0],\n",
        "      embedding_size=word_vectors.vectors.shape[1],\n",
        "      filter_sizes=(3,4,5),\n",
        "      num_filter=128\n",
        "    ).to(DEVICE)\n",
        "    model.embed = torch.nn.Embedding.from_pretrained(\n",
        "      embeddings=torch.as_tensor(word_vectors.vectors, dtype=torch.float32),\n",
        "      freeze=True\n",
        "    ).to(DEVICE)\n",
        "    # model = SentimentLSTM(\n",
        "    #   word_vectors=word_vectors.vectors,\n",
        "    #   num_classes=len(set(train_set.labels)),\n",
        "    #   hidden_size=256,\n",
        "    #   num_layers=2\n",
        "    # ).to(DEVICE)\n",
        "    # model = SentimentLSTM2(\n",
        "    #   word_vectors=word_vectors.vectors,\n",
        "    #   num_classes=len(set(train_set.labels)),\n",
        "    #   hidden_size=256,\n",
        "    #   num_layers=2\n",
        "    # ).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, SCH_STEP_SIZE, SCH_GAMMA)\n",
        "    \n",
        "    # Prepare plot\n",
        "    validation_results = np.full((NB_EPOCHS, 3), np.nan)\n",
        "    plot_results_to_grid(\n",
        "      grid, (grid.rows-2 - ix, 0), branch,\n",
        "      validation_results[:, 1], \"validation\", 'g',\n",
        "      validation_results[:, 2], 'b'\n",
        "    )\n",
        "    # Train & vaildate\n",
        "    for epoch in range(1, NB_EPOCHS + 1):\n",
        "      # Display training output\n",
        "      with grid.output_to(grid.rows-1, 0):\n",
        "        TrainNN(model, \"LSTM\", DEVICE, train_loader, optimizer, epoch, math.ceil(len(train_loader) / 100))\n",
        "        validation_results[epoch-1, 0:2] = ValidateNN(model, \"LSTM\", DEVICE, valid_loader)\n",
        "        validation_results[epoch-1, 2] = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step()\n",
        "    \n",
        "      plot_results_to_grid(\n",
        "        grid, (grid.rows-2 - ix, 0), branch,\n",
        "        validation_results[:, 1], \"validation\", 'g',\n",
        "        validation_results[:, 2], 'b'\n",
        "      )\n",
        "    \n",
        "    # Test\n",
        "    with grid.output_to(grid.rows-2 - ix, 0):\n",
        "      test_results = TestNN(model, \"LSTM\", DEVICE, test_loader)\n",
        "    \n",
        "    if binary:\n",
        "      binary = \"binary_\"\n",
        "    else:\n",
        "      binary = \"\"\n",
        "    \n",
        "    # Save\n",
        "    joblib.dump(\n",
        "      (model.cpu(), validation_results, test_results),\n",
        "      os.path.join(\n",
        "        drive_folder,\n",
        "        \"SentimentLSTM_{}{}.joblib\".format(binary, branch.replace('.', '_'))\n",
        "      )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_dOc-j0NCZ8"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import joblib\n",
        "\n",
        "branches = \\\n",
        "[\n",
        "  \"amazon.com\",\n",
        "  \"amazon.fr\",\n",
        "  \"translated.amazon.fr\",\n",
        "  \"amazon.es\",\n",
        "  \"translated.amazon.es\",\n",
        "  \"amazon.cn\",\n",
        "  \"translated.amazon.cn\"\n",
        "]\n",
        "\n",
        "data = OrderedDict()\n",
        "precision = OrderedDict()\n",
        "recall = OrderedDict()\n",
        "f1 = OrderedDict()\n",
        "\n",
        "for branch in branches:\n",
        "  all_results = joblib.load(\n",
        "    os.path.join(\n",
        "      drive_folder, 'SentimentLSTM_{}.joblib'.format(branch.replace('.', '_'))\n",
        "    )\n",
        "  )\n",
        "  test_results = all_results[-1][-1]\n",
        "  data[branch] = torch.zeros((0, 2), dtype=torch.int64)\n",
        "  for result in test_results:\n",
        "    data[branch] = torch.cat([data[branch], torch.stack([result[0].cpu().flatten(), result[1].cpu().flatten()], dim=1)], dim=0)\n",
        "  \n",
        "  num_classes = len(set(data[branch].flatten().numpy()))\n",
        "\n",
        "  precision[branch] = np.array([\n",
        "    float \\\n",
        "    (\n",
        "      torch.sum((data[branch][:,0] == rating) * (data[branch][:,1] == rating)) \n",
        "      / torch.sum((data[branch][:,0] == rating)).to(torch.float32)\n",
        "    )\n",
        "    for rating in range(num_classes)\n",
        "  ])\n",
        "\n",
        "  recall[branch] = np.array([\n",
        "    float \\\n",
        "    (\n",
        "      torch.sum((data[branch][:,0] == rating) * (data[branch][:,1] == rating)) \n",
        "      / torch.sum((data[branch][:,1] == rating)).to(torch.float32)\n",
        "    )\n",
        "    for rating in range(num_classes)\n",
        "  ])\n",
        "\n",
        "  f1[branch] = 2*precision[branch]*recall[branch]/(precision[branch]+recall[branch])\n",
        "\n",
        "display(list(precision.items()))\n",
        "display(list(recall.items()))\n",
        "display(list(f1.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOUrSDrh_CAJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "cellView": "code",
        "outputId": "752432db-cd39-450a-dd9e-cf3e34042e77"
      },
      "source": [
        "# @title Experiment #2\n",
        "from collections import OrderedDict\n",
        "from A0_utils import TestNN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "branches = \\\n",
        "[\n",
        "  \"translated.amazon.fr\",\n",
        "  \"translated.amazon.es\",\n",
        "  \"translated.amazon.cn\"\n",
        "]\n",
        "\n",
        "data = OrderedDict()\n",
        "precision = OrderedDict()\n",
        "recall = OrderedDict()\n",
        "f1 = OrderedDict()\n",
        "results = pd.DataFrame(data=None, index=[\"5\", \"4\", \"3\", \"2\", \"1\", \"+\", \"-\"], columns=branches)\n",
        "\n",
        "for ix, branch in enumerate(branches):\n",
        "  precision[branch] = [[], []]\n",
        "  recall[branch] = [[], []]\n",
        "  f1[branch] = [[], []]\n",
        "  for binary in [False, True]:\n",
        "    # Load model\n",
        "    model = joblib.load(\n",
        "      os.path.join(\n",
        "        drive_folder,\n",
        "        'SentimentLSTM_{}amazon_com.joblib'.format(\"binary_\" if binary else \"\")\n",
        "      )\n",
        "    )\n",
        "    model = model[0].to(DEVICE)\n",
        "    # Load test set\n",
        "    index = branch.find('.amazon')\n",
        "    prefix = (branch[:index] + \"_\") if index != -1 else ''\n",
        "    branch_name = branch[1+index:]\n",
        "    test_set = CsvDataset(\n",
        "      \"input/{prefix}train_valid_test/{prefix}test_{branch}.csv\".format(\n",
        "          prefix=prefix, branch=branch_name.replace('.', '_')\n",
        "      ), lambda c: c[\"preprocessed\"].split(), lambda c: int(c[\"labels\"])\n",
        "    )\n",
        "    # Load English word vectors\n",
        "    word_vectors = gensim.models.KeyedVectors.load(\n",
        "      \"input/gensim/amazon-com.wv\", mmap='r'\n",
        "    )\n",
        "    word_vectors.add('<unk>', np.full(word_vectors.vector_size, 0))\n",
        "    # Find longest document in training set.\n",
        "    longest_document_length = 402  # longuest amazon_com document\n",
        "    # Word to word vector index (the->1, office->223, ...)\n",
        "    Word2Ix(test_set, word_vectors, longest_document_length)\n",
        "    if binary:\n",
        "      # Binarize labels\n",
        "      test_set.labels = [int(label == 5) for label in test_set.labels]\n",
        "    else:\n",
        "      # Labels must be 0-4 rather than 1-5\n",
        "      test_set.labels = [label-1 for label in test_set.labels]\n",
        "    # Loader\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    test_results = TestNN(model, \"LSTM\", DEVICE, test_loader)\n",
        "    test_results = test_results[-1]\n",
        "    data = torch.zeros((0, 2), dtype=torch.int64)\n",
        "    for result in test_results:\n",
        "      data = torch.cat([data, torch.stack([result[0].cpu().flatten(), result[1].cpu().flatten()], dim=1)], dim=0)\n",
        "    \n",
        "    num_classes = len(set(data.flatten().numpy()))\n",
        "  \n",
        "    precision[branch][binary] = np.array([\n",
        "      float \\\n",
        "      (\n",
        "        torch.sum((data[:,0] == rating) * (data[:,1] == rating)) \n",
        "        / torch.sum((data[:,0] == rating)).to(torch.float32)\n",
        "      )\n",
        "      for rating in range(num_classes)\n",
        "    ])\n",
        "  \n",
        "    recall[branch][binary] = np.array([\n",
        "      float \\\n",
        "      (\n",
        "        torch.sum((data[:,0] == rating) * (data[:,1] == rating)) \n",
        "        / torch.sum((data[:,1] == rating)).to(torch.float32)\n",
        "      )\n",
        "      for rating in range(num_classes)\n",
        "    ])\n",
        "  \n",
        "    f1[branch][binary] = 2*precision[branch][binary]*recall[branch][binary]/(precision[branch][binary]+recall[branch][binary])\n",
        "  \n",
        "  for jx, name in enumerate(results.index[:5]):\n",
        "    results.loc[name, results.columns[ix]] = f1[branch][False][jx]\n",
        "  \n",
        "  results.iloc[-2:, ix] = f1[branch][True]\n",
        "\n",
        "display(results.applymap(lambda x: round(100*x, 2)))\n",
        "\n",
        "# display(list(precision.items()))\n",
        "# display(list(recall.items()))\n",
        "# display(list(f1.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "{LSTM} Test set: Average loss: 0.8571, Accuracy: 696/1000 (70%)\n",
            "\n",
            "\n",
            "{LSTM} Test set: Average loss: 0.5160, Accuracy: 794/1000 (79%)\n",
            "\n",
            "\n",
            "{LSTM} Test set: Average loss: 0.8040, Accuracy: 717/1000 (72%)\n",
            "\n",
            "\n",
            "{LSTM} Test set: Average loss: 0.5160, Accuracy: 802/1000 (80%)\n",
            "\n",
            "\n",
            "{LSTM} Test set: Average loss: 0.9952, Accuracy: 644/1000 (64%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "{LSTM} Test set: Average loss: 0.5919, Accuracy: 722/1000 (72%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>translated.amazon.fr</th>\n",
              "      <th>translated.amazon.es</th>\n",
              "      <th>translated.amazon.cn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57.89</td>\n",
              "      <td>61.63</td>\n",
              "      <td>40.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14.81</td>\n",
              "      <td>8.89</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.43</td>\n",
              "      <td>23.08</td>\n",
              "      <td>18.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25.73</td>\n",
              "      <td>22.94</td>\n",
              "      <td>24.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85.11</td>\n",
              "      <td>85.56</td>\n",
              "      <td>82.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>+</th>\n",
              "      <td>68.31</td>\n",
              "      <td>66.44</td>\n",
              "      <td>63.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>84.74</td>\n",
              "      <td>85.96</td>\n",
              "      <td>77.51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   translated.amazon.fr  translated.amazon.es  translated.amazon.cn\n",
              "5                 57.89                 61.63                 40.68\n",
              "4                 14.81                  8.89                   NaN\n",
              "3                 11.43                 23.08                 18.54\n",
              "2                 25.73                 22.94                 24.43\n",
              "1                 85.11                 85.56                 82.34\n",
              "+                 68.31                 66.44                 63.61\n",
              "-                 84.74                 85.96                 77.51"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}